
import os
import numpy as np
import pandas as pd
from config import DATASET_DIR, remove_classes

train_folder = os.path.join(DATASET_DIR, "TRAIN")
test_folder = os.path.join(DATASET_DIR, "TEST")
train_labels = pd.read_csv(os.path.join(DATASET_DIR, "train_labels.csv"))
test_labels = pd.read_csv(os.path.join(DATASET_DIR, "test_labels.csv"))

cls_ = set(train_labels["class"]) - set(remove_classes)

train_labels = train_labels[train_labels["class"].isin(cls_)]
test_labels = test_labels[test_labels["class"].isin(cls_)]

print(len(os.listdir(train_folder)), len(os.listdir(test_folder)))
print(train_labels.shape, test_labels.shape)
print(len(train_labels.filename.unique()), len(test_labels.filename.unique()))
4720 476
(8139, 8) (430, 8)
2119 214
from helper_functions import add_coco_annot

train_labels = add_coco_annot(train_labels)
test_labels = add_coco_annot(test_labels)
import seaborn as sn
import matplotlib.pyplot as plt

sn.histplot(train_labels["class"])
plt.xticks(rotation=90)
([0,
  1,
  2,
  3,
  4,
  5,
  6,
  7,
  8,
  9,
  10,
  11,
  12,
  13,
  14,
  15,
  16,
  17,
  18,
  19,
  20,
  21,
  22,
  23],
 [Text(0, 0, 'Cherry leaf'),
  Text(1, 0, 'Peach leaf'),
  Text(2, 0, 'Corn leaf blight'),
  Text(3, 0, 'Apple rust leaf'),
  Text(4, 0, 'Potato leaf late blight'),
  Text(5, 0, 'Strawberry leaf'),
  Text(6, 0, 'Tomato leaf late blight'),
  Text(7, 0, 'Tomato mold leaf'),
  Text(8, 0, 'Potato leaf early blight'),
  Text(9, 0, 'Apple leaf'),
  Text(10, 0, 'Tomato leaf yellow virus'),
  Text(11, 0, 'Blueberry leaf'),
  Text(12, 0, 'Tomato leaf mosaic virus'),
  Text(13, 0, 'Raspberry leaf'),
  Text(14, 0, 'Tomato leaf bacterial spot'),
  Text(15, 0, 'Squash Powdery mildew leaf'),
  Text(16, 0, 'grape leaf'),
  Text(17, 0, 'Tomato Early blight leaf'),
  Text(18, 0, 'Apple Scab Leaf'),
  Text(19, 0, 'Tomato Septoria leaf spot'),
  Text(20, 0, 'Tomato leaf'),
  Text(21, 0, 'Soyabean leaf'),
  Text(22, 0, 'Bell_pepper leaf spot'),
  Text(23, 0, 'Bell_pepper leaf')])

 
from PIL import Image

filename = "dsc_8151s.jpg"
img = Image.open(train_folder + f"/{filename}")
img = img.convert("RGB")

img
from helper_functions import draw_bbox

annotations = train_labels[train_labels.filename==filename][['class', 'xmin', 'ymin', 'width', 'height']]
img = draw_bbox(img, annotations.values.tolist())

img
 
train_labels[train_labels.filename=="powdery-mildew-on-squash-leaves.jpg"]
image_id	filename	image_height	image_width	class	class_label	xmin	ymin	width	height
8269	2060	powdery-mildew-on-squash-leaves.jpg	454	680	Squash Powdery mildew leaf	15	323	57	357	385
8270	2060	powdery-mildew-on-squash-leaves.jpg	454	680	Squash Powdery mildew leaf	15	7	1	317	262
8271	2060	powdery-mildew-on-squash-leaves.jpg	454	680	Squash Powdery mildew leaf	15	77	170	270	284
train_labels[train_labels.image_id==2057]
image_id	filename	image_height	image_width	class	class_label	xmin	ymin	width	height
8257	2057	1058_0.jpeg?itok=OQkdtxgv.jpg	600	800	Tomato Early blight leaf	17	208	26	155	76
8258	2057	1058_0.jpeg?itok=OQkdtxgv.jpg	600	800	Tomato Early blight leaf	17	1	183	799	412
 
from datasets import Dataset, Image

filenames = ['cherry-tree-leaves-and-fruits.jpg', 'peach-and-leaf-stock-image-2809275.jpg', 'foodjuly2011+026.jpg', 'applerust-500x383.jpg']

image_ids, images, widths, heights, objects, categories = [], [], [], [], [], []
for filename in filenames:
    image_id = train_labels[train_labels.filename==filename]["image_id"].values[0]
    image = train_folder + f"/{filename}"
    width = train_labels[train_labels.filename==filename]["image_width"].values[0]
    height = train_labels[train_labels.filename==filename]["image_height"].values[0]
    
    areas = np.array(train_labels[train_labels.filename==filename]['width'] * train_labels[train_labels.filename==filename]['height']).tolist()
    bboxes = train_labels[train_labels.filename==filename][['xmin', 'ymin', 'width', 'height']].values.tolist()
    category = train_labels[train_labels.filename==filename]['class_label'].values.tolist()
    object = {'area': areas, 'bbox': bboxes, 'category': category}
    
    image_ids.append(image_id)
    images.append(image)
    widths.append(width)
    heights.append(height)
    objects.append(object)
    
dummy_ds = Dataset.from_dict({"image_id": image_ids, "image": images, "width": widths, "height": heights, "objects": objects})
dummy_ds = dummy_ds.cast_column("image", Image())
from transformers import AutoImageProcessor

checkpoint = "facebook/detr-resnet-50"
image_processor = AutoImageProcessor.from_pretrained(checkpoint)
Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.
from datasets import DatasetDict
dummy_ds2 = DatasetDict({"train": Dataset.from_dict({"image_id": image_ids, "image": images, "width": widths, "height": heights, "objects": objects}),
             "test": Dataset.from_dict({"image_id": image_ids, "image": images, "width": widths, "height": heights, "objects": objects}),
             })
train_labels[train_labels.filename.isin(['tomato-early-blight-1'])
image_id	filename	image_height	image_width	class	class_label	xmin	ymin	width	height
train_labels
image_id	filename	image_height	image_width	class	class_label	xmin	ymin	width	height
0	0	cherry-tree-leaves-and-fruits.jpg	300	350	Cherry leaf	0	198	77	101	175
1	0	cherry-tree-leaves-and-fruits.jpg	300	350	Cherry leaf	0	3	114	145	121
2	0	cherry-tree-leaves-and-fruits.jpg	300	350	Cherry leaf	0	30	184	159	113
3	0	cherry-tree-leaves-and-fruits.jpg	300	350	Cherry leaf	0	226	4	120	79
4	1	peach-and-leaf-stock-image-2809275.jpg	1099	1300	Peach leaf	1	237	479	290	331
...	...	...	...	...	...	...	...	...	...	...
8464	2116	pddl-highlights-fig-1-bact-spot.jpg	1000	1500	Tomato leaf bacterial spot	14	17	204	254	229
8465	2116	pddl-highlights-fig-1-bact-spot.jpg	1000	1500	Tomato leaf bacterial spot	14	279	180	180	309
8466	2116	pddl-highlights-fig-1-bact-spot.jpg	1000	1500	Tomato leaf bacterial spot	14	588	267	343	473
8467	2117	bacterialLeafSpot07-2jqdlmz.jpg	3000	4000	Tomato leaf bacterial spot	14	1065	512	1907	1645
8468	2118	2159_0.jpeg?itok=eBFRbolm.jpg	385	256	Corn leaf blight	2	39	1	169	367
8139 rows Ã— 10 columns

 
import albumentations
import numpy as np
import torch

transform = albumentations.Compose(
    [
        albumentations.Resize(480, 480),
        albumentations.HorizontalFlip(p=1.0),
        albumentations.RandomBrightnessContrast(p=1.0),
    ],
    bbox_params=albumentations.BboxParams(format="coco", label_fields=["category"]),
)
 
[[932, 352, 164, 203],
 [455, 107, 172, 238],
 [577, 326, 186, 215],
 [364, 504, 265, 99],
 [224, 567, 235, 210],
 [65, 756, 277, 289],
 [467, 728, 238, 322],
 [643, 590, 253, 141],
 [923, 519, 369, 308],
 [918, 678, 206, 271]]
for i in range(max(train_labels.image_id.values)):
    try:
        opt = transform(image=np.array(Image.open(train_folder + "/" + train_labels[train_labels.image_id==i]["filename"].values[0])), 
                    bboxes=train_labels[train_labels.image_id==i][['xmin', 'ymin', 'width', 'height']].values.tolist(), 
                    category=train_labels[train_labels.image_id==i]["class_label"].values.tolist())
    except FileNotFoundError:
        pass
    except:
        print(train_labels[train_labels.image_id==i]["filename"].values[0])
IMG_1526.jpg
Early-blight.jpg
tomato_plants_1_original.JPG?1407178095.jpg
IMG_2348.jpg
tomato-septoria-3.jpg
for i in range(max(test_labels.image_id.values)):
    try:
        opt = transform(image=np.array(Image.open(test_folder + "/" + test_labels[test_labels.image_id==i]["filename"].values[0])), 
                    bboxes=test_labels[test_labels.image_id==i][['xmin', 'ymin', 'width', 'height']].values.tolist(), 
                    category=test_labels[test_labels.image_id==i]["class_label"].values.tolist())
    except FileNotFoundError:
        pass
    except:
        print(test_labels[test_labels.image_id==i]["filename"].values[0])
 
plt.imshow(opt["image"])#.shape
<matplotlib.image.AxesImage at 0x7f1c3ff194c0>

 
def formatted_anns(image_id, category, area, bbox):
    annotations = []
    for i in range(0, len(category)):
        new_ann = {
            "image_id": image_id,
            "category_id": category[i],
            "isCrowd": 0,
            "area": area[i],
            "bbox": list(bbox[i]),
        }
        annotations.append(new_ann)

    return annotations
# transforming a batch
def transform_aug_ann(examples):
    image_ids = examples["image_id"]
    images, bboxes, area, categories = [], [], [], []
    for image, objects in zip(examples["image"], examples["objects"]):
        image = np.array(image.convert("RGB"))[:, :, ::-1]
        out = transform(image=image, bboxes=objects["bbox"], category=objects["category"])

        area.append(objects["area"])
        images.append(out["image"])
        bboxes.append(out["bboxes"])
        categories.append(out["category"])

    targets = [
        {"image_id": id_, "annotations": formatted_anns(id_, cat_, ar_, box_)}
        for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)
    ]

    return image_processor(images=images, annotations=targets, return_tensors="pt")
dummy_ds = dummy_ds.map(transform_aug_ann, batched=True, batch_size=2)
Map:   0%|          | 0/4 [00:00<?, ? examples/s]
{'image_id': [0, 1], 'image': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=350x300 at 0x7F03E67BB100>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1300x1099 at 0x7F03868DFA60>], 'width': [350, 1300], 'height': [300, 1099], 'objects': [{'area': [17675, 17545, 17967, 9480], 'bbox': [[198, 77, 101, 175], [3, 114, 145, 121], [30, 184, 159, 113], [226, 4, 120, 79]], 'category': [0, 0, 0, 0]}, {'area': [95990, 187680, 109292, 124584], 'bbox': [[237, 479, 290, 331], [105, 520, 480, 391], [344, 106, 307, 356], [724, 106, 179, 696]], 'category': [1, 1, 1, 1]}]}
[{'image_id': 0, 'annotations': [{'image_id': 0, 'category_id': 0, 'isCrowd': 0, 'area': 17675, 'bbox': [69.94285714285712, 123.19999999999999, 138.51428571428573, 280.0]}, {'image_id': 0, 'category_id': 0, 'isCrowd': 0, 'area': 17545, 'bbox': [277.02857142857147, 182.4, 198.85714285714283, 193.6]}, {'image_id': 0, 'category_id': 0, 'isCrowd': 0, 'area': 17967, 'bbox': [220.79999999999998, 294.4, 218.05714285714285, 180.8]}, {'image_id': 0, 'category_id': 0, 'isCrowd': 0, 'area': 9480, 'bbox': [5.485714285714298, 6.4, 164.57142857142858, 126.4]}]}, {'image_id': 1, 'annotations': [{'image_id': 1, 'category_id': 1, 'isCrowd': 0, 'area': 95990, 'bbox': [285.4153846153846, 209.2083712465878, 107.0769230769231, 144.5677888989991]}, {'image_id': 1, 'category_id': 1, 'isCrowd': 0, 'area': 187680, 'bbox': [264.0, 227.11555959963604, 177.23076923076923, 170.77343039126478]}, {'image_id': 1, 'category_id': 1, 'isCrowd': 0, 'area': 109292, 'bbox': [239.63076923076923, 46.29663330300273, 113.35384615384609, 155.48680618744314]}, {'image_id': 1, 'category_id': 1, 'isCrowd': 0, 'area': 124584, 'bbox': [146.58461538461538, 46.29663330300273, 66.09230769230768, 303.98544131028206]}]}]
{'image_id': [2, 4], 'image': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1011x804 at 0x7F03E67BB040>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x383 at 0x7F038689A7F0>], 'width': [1011, 500], 'height': [804, 383], 'objects': [{'area': [261516, 215441], 'bbox': [[44, 217, 703, 372], [77, 137, 323, 667]], 'category': [1, 1]}, {'area': [90344, 37668], 'bbox': [[1, 3, 491, 184], [36, 205, 292, 129]], 'category': [3, 3]}]}
[{'image_id': 2, 'annotations': [{'image_id': 2, 'category_id': 1, 'isCrowd': 0, 'area': 261516, 'bbox': [125.3412462908012, 129.55223880597015, 333.76854599406533, 222.089552238806]}, {'image_id': 2, 'category_id': 1, 'isCrowd': 0, 'area': 215441, 'bbox': [290.0890207715134, 81.7910447761194, 153.35311572700294, 398.2089552238806]}]}, {'image_id': 4, 'annotations': [{'image_id': 4, 'category_id': 3, 'isCrowd': 0, 'area': 90344, 'bbox': [7.680000000000007, 3.7597911227154044, 471.36, 230.6005221932115]}, {'image_id': 4, 'category_id': 3, 'isCrowd': 0, 'area': 37668, 'bbox': [165.11999999999998, 256.9190600522193, 280.32000000000005, 161.67101827676242]}]}]
dummy_ds2 = dummy_ds2.with_transform(transform_aug_ann)
dummy_ds2
DatasetDict({
    train: Dataset({
        features: ['image_id', 'image', 'width', 'height', 'objects'],
        num_rows: 4
    })
    test: Dataset({
        features: ['image_id', 'image', 'width', 'height', 'objects'],
        num_rows: 4
    })
})
 
{'image_id': 15,
 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=943x663 at 0x7F9EC9E77C10>,
 'width': 943,
 'height': 663,
 'objects': {'id': [114, 115, 116, 117],
  'area': [3796, 1596, 152768, 81002],
  'bbox': [[302.0, 109.0, 73.0, 52.0],
   [810.0, 100.0, 57.0, 28.0],
   [160.0, 31.0, 248.0, 616.0],
   [741.0, 68.0, 202.0, 401.0]],
  'category': [4, 4, 0, 0]}}
sn.histplot(train_labels["image_height"])
<Axes: xlabel='image_height', ylabel='Count'>

train_labels["image_height"].max(), train_labels["image_height"].min()
(6000, 85)
 
from datasets import load_dataset

ds = load_dataset("susnato/plant_disease_detection_processed")
Resolving data files:   0%|          | 0/56 [00:00<?, ?it/s]
Resolving data files:   0%|          | 0/56 [00:00<?, ?it/s]
 
ds['train'][0]
ds['train'].features['objects']['category']
Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)
ds['train']
Dataset({
    features: ['image_id', 'image', 'width', 'height', 'objects', 'pixel_values', 'pixel_mask', 'labels'],
    num_rows: 2110
})
import numpy as np
np.array(ds['train'][0]['pixel_values']).shape
(3, 800, 800)
import os
import pandas as pd
from config import DATASET_DIR, remove_classes
from helper_functions import rename_files, add_coco_annot

train_labels2 = pd.read_csv(os.path.join(DATASET_DIR, "train_labels.csv"))
test_labels2 = pd.read_csv(os.path.join(DATASET_DIR, "test_labels.csv"))

train_labels2 = rename_files(train_labels2)
test_labels2 = rename_files(test_labels2)

cls_ = set(train_labels2["class"]) - set(remove_classes)

train_labels2 = train_labels2[train_labels2["class"].isin(cls_)]
test_labels2 = test_labels2[test_labels2["class"].isin(cls_)]

train_labels2 = add_coco_annot(train_labels2)
test_labels2 = add_coco_annot(test_labels2)
{'Cherry leaf': 0, 'Peach leaf': 1, 'Corn leaf blight': 2, 'Apple rust leaf': 3, 'Potato leaf late blight': 4, 'Strawberry leaf': 5, 'Tomato leaf late blight': 6, 'Tomato mold leaf': 7, 'Potato leaf early blight': 8, 'Apple leaf': 9, 'Tomato leaf yellow virus': 10, 'Blueberry leaf': 11, 'Tomato leaf mosaic virus': 12, 'Raspberry leaf': 13, 'Tomato leaf bacterial spot': 14, 'Squash Powdery mildew leaf': 15, 'grape leaf': 16, 'Tomato Early blight leaf': 17, 'Apple Scab Leaf': 18, 'Tomato Septoria leaf spot': 19, 'Tomato leaf': 20, 'Soyabean leaf': 21, 'Bell_pepper leaf spot': 22, 'Bell_pepper leaf': 23}
{'Cherry leaf': 0, 'Peach leaf': 1, 'Corn leaf blight': 2, 'Apple rust leaf': 3, 'Potato leaf late blight': 4, 'Strawberry leaf': 5, 'Tomato leaf late blight': 6, 'Tomato mold leaf': 7, 'Potato leaf early blight': 8, 'Apple leaf': 9, 'Tomato leaf yellow virus': 10, 'Blueberry leaf': 11, 'Tomato leaf mosaic virus': 12, 'Raspberry leaf': 13, 'Tomato leaf bacterial spot': 14, 'Squash Powdery mildew leaf': 15, 'grape leaf': 16, 'Tomato Early blight leaf': 17, 'Apple Scab Leaf': 18, 'Tomato Septoria leaf spot': 19, 'Tomato leaf': 20, 'Soyabean leaf': 21, 'Bell_pepper leaf spot': 22, 'Bell_pepper leaf': 23}
traind = train_labels2[["class", "class_label"]].drop_duplicates()
testd = train_labels2[["class", "class_label"]].drop_duplicates()
traind.values
array([['Cherry leaf', 0],
       ['Peach leaf', 1],
       ['Corn leaf blight', 2],
       ['Apple rust leaf', 3],
       ['Potato leaf late blight', 4],
       ['Strawberry leaf', 5],
       ['Tomato leaf late blight', 6],
       ['Tomato mold leaf', 7],
       ['Potato leaf early blight', 8],
       ['Apple leaf', 9],
       ['Tomato leaf yellow virus', 10],
       ['Blueberry leaf', 11],
       ['Tomato leaf mosaic virus', 12],
       ['Raspberry leaf', 13],
       ['Tomato leaf bacterial spot', 14],
       ['Squash Powdery mildew leaf', 15],
       ['grape leaf', 16],
       ['Tomato Early blight leaf', 17],
       ['Apple Scab Leaf', 18],
       ['Tomato Septoria leaf spot', 19],
       ['Tomato leaf', 20],
       ['Soyabean leaf', 21],
       ['Bell_pepper leaf spot', 22],
       ['Bell_pepper leaf', 23]], dtype=object)
testd.values
array([['Cherry leaf', 0],
       ['Peach leaf', 1],
       ['Corn leaf blight', 2],
       ['Apple rust leaf', 3],
       ['Potato leaf late blight', 4],
       ['Strawberry leaf', 5],
       ['Tomato leaf late blight', 6],
       ['Tomato mold leaf', 7],
       ['Potato leaf early blight', 8],
       ['Apple leaf', 9],
       ['Tomato leaf yellow virus', 10],
       ['Blueberry leaf', 11],
       ['Tomato leaf mosaic virus', 12],
       ['Raspberry leaf', 13],
       ['Tomato leaf bacterial spot', 14],
       ['Squash Powdery mildew leaf', 15],
       ['grape leaf', 16],
       ['Tomato Early blight leaf', 17],
       ['Apple Scab Leaf', 18],
       ['Tomato Septoria leaf spot', 19],
       ['Tomato leaf', 20],
       ['Soyabean leaf', 21],
       ['Bell_pepper leaf spot', 22],
       ['Bell_pepper leaf', 23]], dtype=object)
# traind = dict((k, v) for k, v in zip(train_labels2["class"].unique(), range(len(train_labels2["class"].unique()))))
# testd = dict((k, v) for k, v in zip(test_labels2["class"].unique(), range(len(test_labels2["class"].unique()))))
set(testd.keys()) - set(traind.keys())
set()
set(traind.keys()) - set(testd.keys())
set()
sorted(traind.items(), key=lambda x: x[0])
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[31], line 1
----> 1 sorted(traind.items(), key=lambda x: x[0]).values()

AttributeError: 'list' object has no attribute 'values'
sorted(testd.items(), key=lambda x: x[0])
[('class',
  0                     Cherry leaf
  4                      Peach leaf
  10               Corn leaf blight
  12                Apple rust leaf
  14        Potato leaf late blight
  22                Strawberry leaf
  35        Tomato leaf late blight
  36               Tomato mold leaf
  38       Potato leaf early blight
  56                     Apple leaf
  65       Tomato leaf yellow virus
  69                 Blueberry leaf
  81       Tomato leaf mosaic virus
  83                 Raspberry leaf
  88     Tomato leaf bacterial spot
  96     Squash Powdery mildew leaf
  97                     grape leaf
  106      Tomato Early blight leaf
  138               Apple Scab Leaf
  207     Tomato Septoria leaf spot
  218                   Tomato leaf
  226                 Soyabean leaf
  278         Bell_pepper leaf spot
  460              Bell_pepper leaf
  Name: class, dtype: object),
 ('class_label',
  0       0
  4       1
  10      2
  12      3
  14      4
  22      5
  35      6
  36      7
  38      8
  56      9
  65     10
  69     11
  81     12
  83     13
  88     14
  96     15
  97     16
  106    17
  138    18
  207    19
  218    20
  226    21
  278    22
  460    23
  Name: class_label, dtype: int64)]
import torch
torch.save(traind, "./classes.pth")
def collate_fn(batch):
    pixel_values = [item["pixel_values"] for item in batch]
    encoding = image_processor.pad(pixel_values, return_tensors="pt")
    labels = [item["labels"] for item in batch]
    batch = {}
    batch["pixel_values"] = encoding["pixel_values"]
    batch["pixel_mask"] = encoding["pixel_mask"]
    batch["labels"] = labels
    return batch
from datasets import load_dataset

load_dataset("susnato/plant_disease_detection_processed")
Downloading readme:   0%|          | 0.00/2.08k [00:00<?, ?B/s]
Resolving data files:   0%|          | 0/56 [00:00<?, ?it/s]
Resolving data files:   0%|          | 0/56 [00:00<?, ?it/s]
DatasetDict({
    train: Dataset({
        features: ['image_id', 'image', 'width', 'height', 'objects', 'pixel_values', 'pixel_mask', 'labels'],
        num_rows: 2110
    })
    test: Dataset({
        features: ['image_id', 'image', 'width', 'height', 'objects', 'pixel_values', 'pixel_mask', 'labels'],
        num_rows: 214
    })
})
 
Evaluation

from datasets import load_dataset

ds = load_dataset("susnato/plant_disease_detection_processed")
Resolving data files:   0%|          | 0/56 [00:00<?, ?it/s]
Resolving data files:   0%|          | 0/56 [00:00<?, ?it/s]
import random
import torch
import json

label2id = torch.load("./classes.pth")
id2label = dict((v, k) for k, v in label2id.items())

# format annotations the same as for training, no need for data augmentation
def val_formatted_anns(image_id, objects):
    annotations = []
    for i in range(0, len(objects["area"])):
        new_ann = {
            "id": (random.getrandbits(8) * i) + i,
            "category_id": objects["category"][i],
            "iscrowd": 0,
            "image_id": image_id[0],
            "area": objects["area"][i],
            "bbox": objects["bbox"][i],
        }
        annotations.append(new_ann)

    return annotations


# Save images and annotations into the files torchvision.datasets.CocoDetection expects
def save_cppe5_annotation_file_images(cppe5):
    output_json = {}
    path_output_cppe5 = f"{os.getcwd()}/test/"

    if not os.path.exists(path_output_cppe5):
        os.makedirs(path_output_cppe5)

    path_anno = os.path.join(path_output_cppe5, "test_ann.json")
    categories_json = [{"supercategory": "none", "id": id, "name": id2label[id]} for id in id2label]
    output_json["images"] = []
    output_json["annotations"] = []
    for example in cppe5:
        ann = val_formatted_anns(example['labels']["image_id"], example["objects"])
        output_json["images"].append(
            {
                "id": example['labels']["image_id"][0],
                "width": example["image"].width,
                "height": example["image"].height,
                "file_name": f"{example['labels']['image_id'][0]}.png",
            }
        )
        output_json["annotations"].extend(ann)
    output_json["categories"] = categories_json

    with open(path_anno, "w") as file:
        json.dump(output_json, file, ensure_ascii=False, indent=4)

    for im, img_id in zip(cppe5["image"], [cppe5['labels'][i]['image_id'] for i in range(len(cppe5))]):
        path_img = os.path.join(path_output_cppe5, f"{img_id[0]}.png")
        im.save(path_img)

    return path_output_cppe5, path_anno
import os
import torchvision
from transformers import AutoImageProcessor

class CocoDetection(torchvision.datasets.CocoDetection):
    def __init__(self, img_folder, image_processor, ann_file):
        super().__init__(img_folder, ann_file)
        self.image_processor = image_processor

    def __getitem__(self, idx):
        # read in PIL image and target in COCO format
        img, target = super(CocoDetection, self).__getitem__(idx)

        # preprocess image and target: converting target to DETR format,
        # resizing + normalization of both image and target)
        image_id = self.ids[idx]
        target = {"image_id": image_id, "annotations": target}
        encoding = self.image_processor(images=img, annotations=target, return_tensors="pt")
        pixel_values = encoding["pixel_values"].squeeze()  # remove batch dimension
        target = encoding["labels"][0]  # remove batch dimension

        return {"pixel_values": pixel_values, "labels": target}


image_processor = AutoImageProcessor.from_pretrained("susnato/detr-resnet-50_finetuned_plant_disease_detection_processed")

path_output_cppe5, path_anno = save_cppe5_annotation_file_images(ds["test"])
test_ds_coco_format = CocoDetection(path_output_cppe5, image_processor, path_anno)
2023-11-26 17:01:16.336368: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
def collate_fn_valid(batch):
    pixel_values = [item["pixel_values"] for item in batch]
    encoding = image_processor.pad(pixel_values, return_tensors="pt")

    labels = {}
    labels_ = batch[0]["labels"]
    for k, v in labels_.items():
        labels[k] = torch.tensor(v)

    batch = {}
    batch["pixel_values"] = encoding["pixel_values"].type(torch.float16)
    batch["pixel_mask"] = encoding["pixel_mask"]
    batch["labels"] = [labels]
    return batch
import evaluate
from tqdm import tqdm
import numpy as np
from pprint import pprint
from transformers import AutoModelForObjectDetection

model = AutoModelForObjectDetection.from_pretrained("susnato/detr-resnet-50_finetuned_plant_disease_detection_processed", 
                                                    torch_dtype=torch.float16,
                                                    _commit_hash="db289d98ee13e78155ffb858f06be08f9cc5f4a5").to("cuda")
module = evaluate.load("ybelkada/cocoevaluate", coco=test_ds_coco_format.coco)
val_dataloader = torch.utils.data.DataLoader(
    test_ds_coco_format, batch_size=1, shuffle=False, num_workers=1, collate_fn=collate_fn_valid
)

with torch.no_grad():
    for idx, batch in enumerate(tqdm(val_dataloader)):
        pixel_values = batch["pixel_values"]
        pixel_mask = batch["pixel_mask"]

        labels = [
            {k: v for k, v in t.items()} for t in batch["labels"]
        ]  # these are in DETR format, resized + normalized

        # forward pass
        outputs = model(pixel_values=pixel_values.to("cuda"), pixel_mask=pixel_mask.to("cuda"))

        orig_target_sizes = torch.stack([target["orig_size"] for target in labels], dim=0)
        result = image_processor.post_process(outputs, orig_target_sizes)[0]  # convert outputs of model to COCO api
        
        results = {}
        for k, v in result.items():
            results[k] = v.cpu()

        module.add(prediction=[results], reference=labels)
        del batch

results = module.compute()
pprint(results['iou_bbox'])
  0%|          | 0/214 [00:00<?, ?it/s]The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.
/tmp/ipykernel_27821/3937569544.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels[k] = torch.tensor(v)
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 214/214 [00:22<00:00,  9.69it/s]
Accumulating evaluation results...
DONE (t=0.14s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.002
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.001
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.014
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.050
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.063
{'AP-IoU=0.50-area=all-maxDets=100': 0.0016482023794021248,
 'AP-IoU=0.50:0.95-area=all-maxDets=100': 0.0009758913561972938,
 'AP-IoU=0.50:0.95-area=large-maxDets=100': 0.0013265169754449922,
 'AP-IoU=0.50:0.95-area=medium-maxDets=100': 0.0,
 'AP-IoU=0.50:0.95-area=small-maxDets=100': 0.0,
 'AP-IoU=0.75-area=all-maxDets=100': 0.0008053363787374018,
 'AR-IoU=0.50:0.95-area=all-maxDets=1': 0.01372222222222222,
 'AR-IoU=0.50:0.95-area=all-maxDets=10': 0.04977777777777778,
 'AR-IoU=0.50:0.95-area=all-maxDets=100': 0.05977777777777778,
 'AR-IoU=0.50:0.95-area=large-maxDets=100': 0.06292397660818713,
 'AR-IoU=0.50:0.95-area=medium-maxDets=100': 0.0,
 'AR-IoU=0.50:0.95-area=small-maxDets=100': 0.0}
 
 
ds['test']
Dataset({
    features: ['image_id', 'image', 'width', 'height', 'objects', 'pixel_values', 'pixel_mask', 'labels'],
    num_rows: 214
})
from PIL import Image
from transformers import pipeline

image = Image.open("./test/71.png")

obj_detector = pipeline("object-detection", model="susnato/detr-resnet-50_finetuned_plant_disease_detection_processed", threshold=0.01)#"")
opt = obj_detector(image)
tensor([[0.0021, 0.0050, 0.0034, 0.0043, 0.0039, 0.0076, 0.0050, 0.0023, 0.0038,
         0.0036, 0.0046, 0.0030, 0.0043, 0.0051, 0.0044, 0.0037, 0.0038, 0.0028,
         0.0032, 0.0041, 0.0045, 0.0027, 0.0026, 0.0044, 0.0039, 0.0050, 0.0030,
         0.0040, 0.0042, 0.0019, 0.0034, 0.0051, 0.0032, 0.0027, 0.0031, 0.0039,
         0.0030, 0.0046, 0.0031, 0.0034, 0.0025, 0.0030, 0.0072, 0.0135, 0.0029,
         0.0072, 0.0028, 0.0057, 0.0027, 0.0032, 0.0031, 0.0034, 0.0065, 0.0032,
         0.0027, 0.0027, 0.0021, 0.0054, 0.0075, 0.0030, 0.0036, 0.0039, 0.0037,
         0.0039, 0.0026, 0.0048, 0.0026, 0.0047, 0.0031, 0.0026, 0.0024, 0.0064,
         0.0055, 0.0050, 0.0040, 0.0043, 0.0034, 0.0020, 0.0031, 0.0028, 0.0026,
         0.0062, 0.0030, 0.0053, 0.0025, 0.0054, 0.0024, 0.0042, 0.0049, 0.0076,
         0.0074, 0.0041, 0.0072, 0.0041, 0.0059, 0.0051, 0.0030, 0.0043, 0.0058,
         0.0048]])
opt
[{'score': 0.013534714467823505,
  'label': 'Soyabean leaf',
  'box': {'xmin': 534, 'ymin': 611, 'xmax': 795, 'ymax': 1113}}]
image

from datasets import load_dataset
from transformers import AutoImageProcessor, AutoModelForObjectDetection

ds = load_dataset("susnato/plant_disease_detection_processed")
image_processor = AutoImageProcessor.from_pretrained("susnato/detr-resnet-50_finetuned_plant_disease_detection_processed")
model = AutoModelForObjectDetection.from_pretrained("susnato/detr-resnet-50_finetuned_plant_disease_detection_processed").to("cuda")
2023-11-26 17:25:29.009410: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Resolving data files:   0%|          | 0/56 [00:00<?, ?it/s]
Resolving data files:   0%|          | 0/56 [00:00<?, ?it/s]
import torch
outputs = model(pixel_values=torch.tensor(ds['train'][0]['pixel_values'], device="cuda")[None])
results = image_processor.post_process_object_detection(outputs, threshold=0.001, target_sizes=[(800, 800, 3, 1)])[0]
tensor([[0.0033, 0.0041, 0.0034, 0.0059, 0.0042, 0.0043, 0.0050, 0.0027, 0.0038,
         0.0043, 0.0069, 0.0036, 0.0054, 0.0032, 0.0038, 0.0028, 0.0037, 0.0031,
         0.0044, 0.0029, 0.0066, 0.0049, 0.0026, 0.0034, 0.0041, 0.0039, 0.0038,
         0.0071, 0.0051, 0.0032, 0.0030, 0.0036, 0.0030, 0.0038, 0.0038, 0.0025,
         0.0044, 0.0032, 0.0036, 0.0033, 0.0037, 0.0043, 0.0063, 0.0049, 0.0032,
         0.0044, 0.0044, 0.0054, 0.0032, 0.0030, 0.0036, 0.0049, 0.0026, 0.0036,
         0.0029, 0.0033, 0.0030, 0.0070, 0.0073, 0.0043, 0.0027, 0.0054, 0.0027,
         0.0030, 0.0029, 0.0023, 0.0030, 0.0032, 0.0017, 0.0028, 0.0026, 0.0041,
         0.0038, 0.0034, 0.0077, 0.0048, 0.0034, 0.0027, 0.0018, 0.0020, 0.0039,
         0.0028, 0.0029, 0.0058, 0.0024, 0.0047, 0.0033, 0.0039, 0.0032, 0.0066,
         0.0053, 0.0052, 0.0077, 0.0050, 0.0038, 0.0061, 0.0031, 0.0063, 0.0041,
         0.0047]], device='cuda:0', grad_fn=<MaxBackward0>)
results
{'scores': tensor([0.0033, 0.0041, 0.0034, 0.0059, 0.0042, 0.0043, 0.0050, 0.0027, 0.0038,
         0.0043, 0.0069, 0.0036, 0.0054, 0.0032, 0.0038, 0.0028, 0.0037, 0.0031,
         0.0044, 0.0029, 0.0066, 0.0049, 0.0026, 0.0034, 0.0041, 0.0039, 0.0038,
         0.0071, 0.0051, 0.0032, 0.0030, 0.0036, 0.0030, 0.0038, 0.0038, 0.0025,
         0.0044, 0.0032, 0.0036, 0.0033, 0.0037, 0.0043, 0.0063, 0.0049, 0.0032,
         0.0044, 0.0044, 0.0054, 0.0032, 0.0030, 0.0036, 0.0049, 0.0026, 0.0036,
         0.0029, 0.0033, 0.0030, 0.0070, 0.0073, 0.0043, 0.0027, 0.0054, 0.0027,
         0.0030, 0.0029, 0.0023, 0.0030, 0.0032, 0.0017, 0.0028, 0.0026, 0.0041,
         0.0038, 0.0034, 0.0077, 0.0048, 0.0034, 0.0027, 0.0018, 0.0020, 0.0039,
         0.0028, 0.0029, 0.0058, 0.0024, 0.0047, 0.0033, 0.0039, 0.0032, 0.0066,
         0.0053, 0.0052, 0.0077, 0.0050, 0.0038, 0.0061, 0.0031, 0.0063, 0.0041,
         0.0047], device='cuda:0', grad_fn=<IndexBackward0>),
 'labels': tensor([10,  9, 17,  9,  9,  5,  9,  5,  5,  5,  1, 10,  1,  1,  1, 10,  5,  1,
          1,  1,  1,  1,  1,  1,  1,  1,  5, 21,  9, 10, 10,  5,  5,  1,  5,  5,
          1,  5,  5,  1,  1,  1,  9,  5, 10,  5,  1,  1, 10,  5,  1,  1,  5,  5,
          5,  5, 10,  9,  9,  1,  5, 21,  1,  5,  5,  5,  5, 10,  5,  5,  1,  1,
          1,  1,  1,  1, 10, 17,  5, 10,  5,  5,  1,  9, 10,  1,  5,  1,  1,  9,
          1,  1,  1,  9,  9,  1, 10,  9,  1,  9], device='cuda:0'),
 'boxes': tensor([[1.1594e+01, 3.1383e+02, 1.5850e+02, 5.8881e+02],
         [3.2099e+02, 1.5805e+02, 4.3900e+02, 3.1748e+02],
         [1.8197e+01, 5.2261e+02, 2.1623e+02, 7.9472e+02],
         [2.9604e+02, 7.2326e+01, 4.5996e+02, 1.7606e+02],
         [2.5568e+02, 5.0020e+01, 4.6507e+02, 1.9351e+02],
         [1.1535e+02, 1.8960e+02, 3.4660e+02, 7.1606e+02],
         [3.4728e+02, 2.6914e+02, 4.5979e+02, 4.6839e+02],
         [1.1534e+02, 2.0991e+02, 2.6666e+02, 6.5301e+02],
         [9.3389e-02, 7.3421e+00, 2.5538e+02, 1.7097e+02],
         [1.7136e+00, 4.6813e+00, 2.4442e+02, 1.7463e+02],
         [3.9495e+02, 4.6838e+02, 7.3084e+02, 7.9621e+02],
         [9.6461e+00, 3.1206e+02, 1.6389e+02, 5.9051e+02],
         [4.6802e+02, 3.1199e+02, 7.8803e+02, 7.9097e+02],
         [4.7422e+02, 3.8404e+02, 7.8328e+02, 7.9036e+02],
         [3.9365e+02, 3.5136e+02, 6.2920e+02, 8.1636e+02],
         [1.3154e+00, 1.1590e+01, 2.6512e+02, 5.9654e+02],
         [2.7738e+00, 5.8073e+00, 2.3623e+02, 1.7003e+02],
         [4.7833e+02, 3.1625e+02, 7.1274e+02, 5.2671e+02],
         [4.8376e+02, 3.1265e+02, 7.8369e+02, 6.4103e+02],
         [1.8887e+01, 1.5247e+02, 7.7966e+02, 7.7947e+02],
         [3.9073e+02, 4.6221e+02, 7.7166e+02, 7.9537e+02],
         [4.5457e+02, 3.1170e+02, 7.8454e+02, 7.9308e+02],
         [2.1020e+01, 1.2616e+02, 7.8062e+02, 7.9009e+02],
         [6.8013e+02, 4.2045e+02, 7.9058e+02, 7.9390e+02],
         [3.2933e+02, 1.5339e+02, 7.7900e+02, 6.7756e+02],
         [3.4488e+02, 1.9049e+02, 7.4213e+02, 7.8966e+02],
         [4.9776e+02, 3.1392e+02, 7.8483e+02, 6.6641e+02],
         [1.1821e+02, 2.0489e+02, 3.4201e+02, 6.9902e+02],
         [2.5554e+02, 5.8782e+01, 4.6327e+02, 1.8129e+02],
         [1.1108e+01, 2.2193e+02, 1.5615e+02, 5.8982e+02],
         [1.3395e+01, 2.5952e+02, 1.6102e+02, 5.9410e+02],
         [2.0496e+02, 2.1027e+02, 3.4403e+02, 7.1687e+02],
         [3.8429e+01, 5.3422e+02, 2.2042e+02, 7.9739e+02],
         [5.0472e+02, 3.1449e+02, 7.8331e+02, 6.5281e+02],
         [4.7443e+02, 3.1275e+02, 7.8429e+02, 7.8868e+02],
         [1.9146e+02, 2.1170e+02, 3.4062e+02, 6.3506e+02],
         [4.8856e+02, 3.1296e+02, 7.8651e+02, 6.6651e+02],
         [1.1441e+02, 1.8954e+02, 3.3359e+02, 6.7521e+02],
         [1.8085e+02, 2.1028e+02, 3.4338e+02, 6.9713e+02],
         [4.6224e+02, 3.0996e+02, 7.0246e+02, 5.7413e+02],
         [4.9700e+02, 3.1373e+02, 7.7944e+02, 6.3335e+02],
         [4.8003e+02, 3.1270e+02, 7.8325e+02, 6.6524e+02],
         [3.1427e+02, 1.5677e+02, 4.7907e+02, 4.8192e+02],
         [1.1305e+02, 1.9062e+02, 3.4437e+02, 7.1972e+02],
         [7.5015e+00, 2.0284e+02, 1.8374e+02, 5.9054e+02],
         [1.1151e+02, 1.5024e+02, 3.4601e+02, 7.0661e+02],
         [4.7047e+02, 3.1163e+02, 7.8167e+02, 6.3177e+02],
         [3.8861e+02, 4.6609e+02, 6.3858e+02, 7.9910e+02],
         [9.1039e+00, 2.1530e+02, 1.6474e+02, 5.9061e+02],
         [1.2468e+02, 2.1120e+02, 3.3724e+02, 6.4244e+02],
         [4.5458e+00, 6.3022e+00, 4.5987e+02, 1.9725e+02],
         [4.6161e+02, 3.0949e+02, 7.8315e+02, 6.5673e+02],
         [5.2851e+01, 5.5482e+02, 2.2590e+02, 7.9526e+02],
         [1.1807e+02, 2.0488e+02, 3.4101e+02, 6.4679e+02],
         [2.8937e+01, 5.2405e+02, 2.1809e+02, 7.9597e+02],
         [1.1516e+02, 2.0450e+02, 3.3150e+02, 6.8605e+02],
         [9.5628e+00, 3.1265e+02, 1.5725e+02, 5.8910e+02],
         [3.2419e+02, 1.5728e+02, 4.6370e+02, 4.7454e+02],
         [3.1370e+02, 1.5475e+02, 4.7394e+02, 4.7734e+02],
         [1.0055e+01, 2.7951e+01, 7.8217e+02, 7.8926e+02],
         [6.5269e+00, 1.1222e+02, 2.7758e+02, 2.0349e+02],
         [1.1007e+02, 1.4875e+02, 3.4267e+02, 7.1676e+02],
         [3.2911e+01, 1.3788e+02, 7.8567e+02, 7.0390e+02],
         [7.0053e+00, 1.1102e+02, 2.7242e+02, 1.8995e+02],
         [1.0722e+01, 1.5792e+01, 4.1480e+02, 1.8891e+02],
         [5.9295e+01, 5.3059e+02, 2.3717e+02, 7.9542e+02],
         [5.1394e+00, 6.5363e+00, 2.4925e+02, 1.7166e+02],
         [1.3006e+00, 1.2296e+02, 2.7383e+02, 5.9651e+02],
         [1.1799e+02, 2.0886e+02, 2.6759e+02, 5.1810e+02],
         [5.7232e+02, 3.1694e+02, 7.8541e+02, 6.6713e+02],
         [5.6040e+02, 3.1742e+02, 7.6706e+02, 5.7909e+02],
         [1.1334e+02, 1.4661e+02, 7.8232e+02, 7.9080e+02],
         [2.3842e+01, 1.4853e+02, 7.8429e+02, 7.8975e+02],
         [4.0068e+02, 3.1001e+02, 7.4215e+02, 7.8687e+02],
         [3.9291e+02, 4.6943e+02, 7.3630e+02, 7.9647e+02],
         [4.5614e+02, 3.0860e+02, 7.8507e+02, 7.0133e+02],
         [6.2455e+00, 1.5785e+02, 2.3699e+02, 5.9623e+02],
         [1.7873e+01, 5.2385e+02, 2.1220e+02, 7.9409e+02],
         [2.2812e+02, 2.1771e+02, 3.3537e+02, 4.7211e+02],
         [9.7964e+00, 2.3347e+02, 1.5237e+02, 5.2186e+02],
         [4.7908e+02, 3.1405e+02, 7.8714e+02, 7.9037e+02],
         [2.1407e+02, 2.1337e+02, 3.4151e+02, 6.2262e+02],
         [5.8094e+02, 3.1371e+02, 7.9322e+02, 7.8689e+02],
         [2.6986e+02, 6.9626e+01, 4.6270e+02, 1.8759e+02],
         [8.2325e+00, 1.8485e+02, 2.5421e+02, 6.0127e+02],
         [4.2263e+02, 5.3915e+02, 7.3465e+02, 7.9965e+02],
         [5.2254e+02, 3.1774e+02, 7.8522e+02, 7.8862e+02],
         [3.2352e+02, 1.5451e+02, 7.8322e+02, 7.9174e+02],
         [1.0274e+02, 2.1103e+02, 7.5521e+02, 7.9161e+02],
         [3.4968e+02, 3.0977e+02, 4.6186e+02, 4.6951e+02],
         [3.2007e+02, 1.5595e+02, 7.5437e+02, 7.9320e+02],
         [4.6772e+02, 3.0931e+02, 7.8833e+02, 6.5922e+02],
         [3.8588e+02, 4.6314e+02, 7.3562e+02, 7.9442e+02],
         [3.2152e+02, 1.5830e+02, 4.7401e+02, 3.6531e+02],
         [3.8313e+02, 2.9008e+02, 5.0291e+02, 4.7787e+02],
         [3.8675e+02, 4.7104e+02, 6.4033e+02, 7.9553e+02],
         [2.8067e+00, 1.2297e+02, 2.6800e+02, 5.9348e+02],
         [2.9553e+02, 7.1049e+01, 4.6098e+02, 1.7971e+02],
         [3.4897e+02, 1.8205e+02, 7.8412e+02, 7.8896e+02],
         [3.2421e+02, 1.5937e+02, 4.7624e+02, 3.2788e+02]], device='cuda:0',
        grad_fn=<IndexBackward0>)}
outputs
DetrObjectDetectionOutput(loss=None, loss_dict=None, logits=tensor([[[-1.4348, -0.4501, -0.6521,  ..., -0.9709, -1.0067,  5.8529],
         [-1.1910, -0.2947, -0.6829,  ..., -0.9079, -0.4279,  5.7711],
         [-1.6047, -0.3406, -0.5027,  ..., -0.8336, -0.7040,  5.7005],
         ...,
         [-1.1694, -0.3344, -0.8705,  ..., -0.7641, -0.5242,  5.2259],
         [-1.8115,  0.1540, -0.7627,  ..., -0.7812, -0.5450,  5.6007],
         [-1.0601, -0.2981, -0.7692,  ..., -0.9452, -0.4153,  5.7382]]],
       device='cuda:0', grad_fn=<ViewBackward0>), pred_boxes=tensor([[[0.1063, 0.5641, 0.1836, 0.3437],
         [0.4750, 0.2972, 0.1475, 0.1993],
         [0.1465, 0.8233, 0.2475, 0.3401],
         [0.4725, 0.1552, 0.2049, 0.1297],
         [0.4505, 0.1522, 0.2617, 0.1794],
         [0.2887, 0.5660, 0.2891, 0.6581],
         [0.5044, 0.4610, 0.1406, 0.2491],
         [0.2388, 0.5393, 0.1891, 0.5539],
         [0.1597, 0.1114, 0.3191, 0.2045],
         [0.1538, 0.1121, 0.3034, 0.2124],
         [0.7036, 0.7904, 0.4199, 0.4098],
         [0.1085, 0.5641, 0.1928, 0.3481],
         [0.7850, 0.6894, 0.4000, 0.5987],
         [0.7859, 0.7340, 0.3863, 0.5079],
         [0.6393, 0.7298, 0.2944, 0.5813],
         [0.1665, 0.3801, 0.3298, 0.7312],
         [0.1494, 0.1099, 0.2918, 0.2053],
         [0.7444, 0.5269, 0.2930, 0.2631],
         [0.7922, 0.5961, 0.3749, 0.4105],
         [0.4991, 0.5825, 0.9510, 0.7837],
         [0.7265, 0.7860, 0.4762, 0.4165],
         [0.7744, 0.6905, 0.4125, 0.6017],
         [0.5010, 0.5727, 0.9495, 0.8299],
         [0.9192, 0.7590, 0.1381, 0.4668],
         [0.6927, 0.5193, 0.5621, 0.6552],
         [0.6794, 0.6126, 0.4966, 0.7490],
         [0.8016, 0.6127, 0.3588, 0.4406],
         [0.2876, 0.5649, 0.2797, 0.6177],
         [0.4493, 0.1500, 0.2597, 0.1531],
         [0.1045, 0.5073, 0.1813, 0.4599],
         [0.1090, 0.5335, 0.1845, 0.4182],
         [0.3431, 0.5795, 0.1738, 0.6332],
         [0.1618, 0.8323, 0.2275, 0.3290],
         [0.8050, 0.6046, 0.3482, 0.4229],
         [0.7867, 0.6884, 0.3873, 0.5949],
         [0.3326, 0.5292, 0.1865, 0.5292],
         [0.7969, 0.6122, 0.3724, 0.4419],
         [0.2800, 0.5405, 0.2740, 0.6071],
         [0.3276, 0.5671, 0.2032, 0.6086],
         [0.7279, 0.5526, 0.3003, 0.3302],
         [0.7978, 0.5919, 0.3530, 0.3995],
         [0.7895, 0.6112, 0.3790, 0.4407],
         [0.4958, 0.3992, 0.2060, 0.4064],
         [0.2859, 0.5690, 0.2891, 0.6614],
         [0.1195, 0.4959, 0.2203, 0.4846],
         [0.2859, 0.5355, 0.2931, 0.6955],
         [0.7826, 0.5896, 0.3890, 0.4002],
         [0.6420, 0.7907, 0.3125, 0.4163],
         [0.1087, 0.5037, 0.1945, 0.4691],
         [0.2887, 0.5335, 0.2657, 0.5390],
         [0.2903, 0.1272, 0.5692, 0.2387],
         [0.7780, 0.6039, 0.4019, 0.4341],
         [0.1742, 0.8438, 0.2163, 0.3006],
         [0.2869, 0.5323, 0.2787, 0.5524],
         [0.1544, 0.8250, 0.2364, 0.3399],
         [0.2792, 0.5566, 0.2704, 0.6019],
         [0.1043, 0.5636, 0.1846, 0.3456],
         [0.4924, 0.3949, 0.1744, 0.3966],
         [0.4923, 0.3951, 0.2003, 0.4032],
         [0.4951, 0.5108, 0.9651, 0.9516],
         [0.1776, 0.1973, 0.3388, 0.1141],
         [0.2830, 0.5409, 0.2907, 0.7100],
         [0.5116, 0.5261, 0.9410, 0.7075],
         [0.1746, 0.1881, 0.3318, 0.0987],
         [0.2660, 0.1279, 0.5051, 0.2164],
         [0.1853, 0.8288, 0.2223, 0.3310],
         [0.1590, 0.1114, 0.3051, 0.2064],
         [0.1720, 0.4497, 0.3407, 0.5919],
         [0.2410, 0.4544, 0.1870, 0.3865],
         [0.8486, 0.6150, 0.2664, 0.4377],
         [0.8297, 0.5603, 0.2583, 0.3271],
         [0.5598, 0.5859, 0.8362, 0.8052],
         [0.5051, 0.5864, 0.9506, 0.8015],
         [0.7143, 0.6855, 0.4268, 0.5961],
         [0.7058, 0.7912, 0.4292, 0.4088],
         [0.7758, 0.6312, 0.4112, 0.4909],
         [0.1520, 0.4713, 0.2884, 0.5480],
         [0.1438, 0.8237, 0.2429, 0.3378],
         [0.3522, 0.4311, 0.1341, 0.3180],
         [0.1014, 0.4721, 0.1782, 0.3605],
         [0.7914, 0.6903, 0.3851, 0.5954],
         [0.3472, 0.5225, 0.1593, 0.5116],
         [0.8589, 0.6879, 0.2653, 0.5915],
         [0.4578, 0.1608, 0.2410, 0.1475],
         [0.1640, 0.4913, 0.3075, 0.5205],
         [0.7233, 0.8368, 0.3900, 0.3256],
         [0.8174, 0.6915, 0.3283, 0.5886],
         [0.6917, 0.5914, 0.5746, 0.7965],
         [0.5362, 0.6267, 0.8156, 0.7257],
         [0.5072, 0.4871, 0.1402, 0.1997],
         [0.6715, 0.5932, 0.5429, 0.7966],
         [0.7850, 0.6053, 0.4008, 0.4374],
         [0.7009, 0.7860, 0.4372, 0.4141],
         [0.4972, 0.3273, 0.1906, 0.2588],
         [0.5538, 0.4800, 0.1497, 0.2347],
         [0.6419, 0.7916, 0.3170, 0.4056],
         [0.1693, 0.4478, 0.3315, 0.5881],
         [0.4728, 0.1567, 0.2068, 0.1358],
         [0.7082, 0.6069, 0.5439, 0.7586],
         [0.5003, 0.3045, 0.1900, 0.2106]]], device='cuda:0',
       grad_fn=<SigmoidBackward0>), auxiliary_outputs=None, last_hidden_state=tensor([[[-1.1066, -1.4558, -1.2631,  ..., -0.4141,  3.9396,  0.1302],
         [-0.8939, -1.1881, -0.1732,  ..., -1.1257,  4.4024,  0.5622],
         [-1.3394, -1.3634, -1.4417,  ..., -0.4391,  4.3254,  0.1819],
         ...,
         [-1.0414, -0.4979, -0.4035,  ..., -0.9262,  3.8240,  0.3697],
         [-0.8149, -1.7522, -0.2179,  ..., -0.6124,  4.3338,  0.4743],
         [-0.6564, -0.7774,  0.0106,  ..., -1.3864,  4.1851,  0.6826]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-7.8460e-02, -2.7334e-02, -1.8309e-02,  ...,  8.9529e-02,
          -7.5883e-02, -9.0845e-02],
         [-7.5514e-02, -2.4489e-02, -1.9645e-02,  ...,  2.1123e-01,
          -1.3289e-01, -1.5784e-01],
         [-6.4079e-02, -2.2744e-02, -1.3033e-02,  ...,  2.2492e-01,
          -1.4292e-01, -2.7653e-01],
         ...,
         [ 7.2938e-02,  1.7785e-04,  1.9452e-02,  ..., -2.4763e-02,
           6.6835e-02,  1.8166e-02],
         [ 5.6846e-02, -4.9366e-03,  8.3705e-03,  ..., -7.0449e-02,
          -1.5803e-01, -6.3004e-02],
         [ 6.0298e-02, -5.2661e-03,  1.3055e-02,  ..., -2.3514e-01,
          -2.6595e-01, -1.0636e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>), encoder_hidden_states=None, encoder_attentions=None)
for i in range(200):
    print(obj_detector(Image.open(f"./test/{i}.png")))
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
[]
---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
Cell In[8], line 2
      1 for i in range(200):
----> 2     print(obj_detector(Image.open(f"./test/{i}.png")))

File ~/temp_files/transformers/src/transformers/pipelines/object_detection.py:104, in ObjectDetectionPipeline.__call__(self, *args, **kwargs)
     72 def __call__(self, *args, **kwargs) -> Union[Predictions, List[Prediction]]:
     73     """
     74     Detect objects (bounding boxes & classes) in the image(s) passed as inputs.
     75 
   (...)
    101         - **box** (`List[Dict[str, int]]`) -- The bounding box of detected object in image's original size.
    102     """
--> 104     return super().__call__(*args, **kwargs)

File ~/temp_files/transformers/src/transformers/pipelines/base.py:1140, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)
   1132     return next(
   1133         iter(
   1134             self.get_iterator(
   (...)
   1137         )
   1138     )
   1139 else:
-> 1140     return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)

File ~/temp_files/transformers/src/transformers/pipelines/base.py:1147, in Pipeline.run_single(self, inputs, preprocess_params, forward_params, postprocess_params)
   1145 def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):
   1146     model_inputs = self.preprocess(inputs, **preprocess_params)
-> 1147     model_outputs = self.forward(model_inputs, **forward_params)
   1148     outputs = self.postprocess(model_outputs, **postprocess_params)
   1149     return outputs

File ~/temp_files/transformers/src/transformers/pipelines/base.py:1046, in Pipeline.forward(self, model_inputs, **forward_params)
   1044     with inference_context():
   1045         model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)
-> 1046         model_outputs = self._forward(model_inputs, **forward_params)
   1047         model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device("cpu"))
   1048 else:

File ~/temp_files/transformers/src/transformers/pipelines/object_detection.py:117, in ObjectDetectionPipeline._forward(self, model_inputs)
    115 def _forward(self, model_inputs):
    116     target_size = model_inputs.pop("target_size")
--> 117     outputs = self.model(**model_inputs)
    118     model_outputs = outputs.__class__({"target_size": target_size, **outputs})
    119     if self.tokenizer is not None:

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File ~/temp_files/transformers/src/transformers/models/detr/modeling_detr.py:1559, in DetrForObjectDetection.forward(self, pixel_values, pixel_mask, decoder_attention_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)
   1556 return_dict = return_dict if return_dict is not None else self.config.use_return_dict
   1558 # First, sent images through DETR base model to obtain encoder + decoder outputs
-> 1559 outputs = self.model(
   1560     pixel_values,
   1561     pixel_mask=pixel_mask,
   1562     decoder_attention_mask=decoder_attention_mask,
   1563     encoder_outputs=encoder_outputs,
   1564     inputs_embeds=inputs_embeds,
   1565     decoder_inputs_embeds=decoder_inputs_embeds,
   1566     output_attentions=output_attentions,
   1567     output_hidden_states=output_hidden_states,
   1568     return_dict=return_dict,
   1569 )
   1571 sequence_output = outputs[0]
   1573 # class logits + predicted bounding boxes

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File ~/temp_files/transformers/src/transformers/models/detr/modeling_detr.py:1394, in DetrModel.forward(self, pixel_values, pixel_mask, decoder_attention_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, output_attentions, output_hidden_states, return_dict)
   1389     pixel_mask = torch.ones(((batch_size, height, width)), device=device)
   1391 # First, sent pixel_values + pixel_mask through Backbone to obtain the features
   1392 # pixel_values should be of shape (batch_size, num_channels, height, width)
   1393 # pixel_mask should be of shape (batch_size, height, width)
-> 1394 features, object_queries_list = self.backbone(pixel_values, pixel_mask)
   1396 # get final feature map and downsampled mask
   1397 feature_map, mask = features[-1]

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File ~/temp_files/transformers/src/transformers/models/detr/modeling_detr.py:403, in DetrConvModel.forward(self, pixel_values, pixel_mask)
    401 def forward(self, pixel_values, pixel_mask):
    402     # send pixel_values and pixel_mask through backbone to get list of (feature_map, pixel_mask) tuples
--> 403     out = self.conv_encoder(pixel_values, pixel_mask)
    404     pos = []
    405     for feature_map, mask in out:
    406         # position encoding

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File ~/temp_files/transformers/src/transformers/models/detr/modeling_detr.py:381, in DetrConvEncoder.forward(self, pixel_values, pixel_mask)
    379 def forward(self, pixel_values: torch.Tensor, pixel_mask: torch.Tensor):
    380     # send pixel_values through the model to get list of feature maps
--> 381     features = self.model(pixel_values) if self.config.use_timm_backbone else self.model(pixel_values).feature_maps
    383     out = []
    384     for feature_map in features:
    385         # downsample pixel_mask to match shape of corresponding feature_map

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/timm/models/_features.py:282, in FeatureListNet.forward(self, x)
    281 def forward(self, x) -> (List[torch.Tensor]):
--> 282     return list(self._collect(x).values())

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/timm/models/_features.py:236, in FeatureDictNet._collect(self, x)
    234     x = module(x) if first_or_last_module else checkpoint(module, x)
    235 else:
--> 236     x = module(x)
    238 if name in self.return_layers:
    239     out_id = self.return_layers[name]

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/torch/nn/modules/container.py:217, in Sequential.forward(self, input)
    215 def forward(self, input):
    216     for module in self:
--> 217         input = module(input)
    218     return input

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/timm/models/resnet.py:200, in Bottleneck.forward(self, x)
    197     x = self.drop_path(x)
    199 if self.downsample is not None:
--> 200     shortcut = self.downsample(shortcut)
    201 x += shortcut
    202 x = self.act3(x)

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/torch/nn/modules/container.py:217, in Sequential.forward(self, input)
    215 def forward(self, input):
    216     for module in self:
--> 217         input = module(input)
    218     return input

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/torch/nn/modules/conv.py:463, in Conv2d.forward(self, input)
    462 def forward(self, input: Tensor) -> Tensor:
--> 463     return self._conv_forward(input, self.weight, self.bias)

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/torch/nn/modules/conv.py:459, in Conv2d._conv_forward(self, input, weight, bias)
    455 if self.padding_mode != 'zeros':
    456     return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),
    457                     weight, bias, self.stride,
    458                     _pair(0), self.dilation, self.groups)
--> 459 return F.conv2d(input, weight, bias, self.stride,
    460                 self.padding, self.dilation, self.groups)

KeyboardInterrupt: 
with torch.no_grad():
    inputs = image_processor(images=image, return_tensors="pt")
    outputs = model(**inputs)
    target_sizes = torch.tensor([image.size[::-1]])
    results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]

for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
    box = [round(i, 2) for i in box.tolist()]
    print(
        f"Detected {model.config.id2label[label.item()]} with confidence "
        f"{round(score.item(), 3)} at location {box}"
    )
 
 
beans = load_dataset("beans")
from transformers import AutoImageProcessor

checkpoint = "google/vit-base-patch16-224"
image_processor = AutoImageProcessor.from_pretrained(checkpoint)
preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]
config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]
from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor

normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)
size = (
    image_processor.size["shortest_edge"]
    if "shortest_edge" in image_processor.size
    else (image_processor.size["height"], image_processor.size["width"])
)
_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])
def transforms(examples):    
    images = [_transforms(image.convert("RGB")) for image in examples['image']]
    labels = [label for label in examples['labels']]

    return {"pixel_values": torch.stack(images), "label": torch.tensor(labels)}
beans = beans.with_transform(transforms)
from transformers import DefaultDataCollator

data_collator = DefaultDataCollator()
import evaluate
import numpy as np

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)
 
os.environ
environ{'PATH': '/home/susnato/anaconda3/envs/phi/bin:/home/susnato/anaconda3/condabin:/home/susnato/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin:/home/susnato/.local/share/JetBrains/Toolbox/scripts',
        'LC_MEASUREMENT': 'en_IN.UTF-8',
        'XAUTHORITY': '/run/user/1000/gdm/Xauthority',
        'INVOCATION_ID': '8c8b0dab9d2c47e78b9a5b6d03d0a70c',
        'XMODIFIERS': '@im=ibus',
        'LC_TELEPHONE': 'en_IN.UTF-8',
        'XDG_DATA_DIRS': '/usr/share/ubuntu:/usr/share/gnome:/home/susnato/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share/:/usr/share/:/var/lib/snapd/desktop',
        'GDMSESSION': 'ubuntu',
        'LC_TIME': 'en_IN.UTF-8',
        'CONDA_DEFAULT_ENV': 'phi',
        'PAPERSIZE': 'a4',
        'GTK_IM_MODULE': 'ibus',
        'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/1000/bus',
        'XDG_CURRENT_DESKTOP': 'ubuntu:GNOME',
        'CONDA_PREFIX': '/home/susnato/anaconda3/envs/phi',
        'JOURNAL_STREAM': '8:30087',
        'LC_PAPER': 'en_IN.UTF-8',
        'SESSION_MANAGER': 'local/susnato-desktop:@/tmp/.ICE-unix/1318,unix/susnato-desktop:/tmp/.ICE-unix/1318',
        'USERNAME': 'susnato',
        'LOGNAME': 'susnato',
        'PWD': '/home/susnato/PycharmProjects/plant_desease_detection',
        'MANAGERPID': '977',
        'TOOLBOX_VERSION': '2.1.1.18388',
        'LANGUAGE': '',
        'GJS_DEBUG_TOPICS': 'JS ERROR;JS LOG',
        'PYTHONPATH': '/home/susnato/PycharmProjects/plant_desease_detection',
        'SHELL': '/bin/bash',
        'LC_ADDRESS': 'en_IN.UTF-8',
        'GIO_LAUNCHED_DESKTOP_FILE': '/home/susnato/.local/share/applications/jetbrains-toolbox.desktop',
        'OLDPWD': '/home/susnato',
        'GNOME_DESKTOP_SESSION_ID': 'this-is-deprecated',
        'GTK_MODULES': 'gail:atk-bridge',
        'LC_ALL': 'en_US.UTF-8',
        'CONDA_PROMPT_MODIFIER': '(phi) ',
        'SYSTEMD_EXEC_PID': '1363',
        'XDG_SESSION_DESKTOP': 'ubuntu',
        'SSH_AGENT_LAUNCHER': 'gnome-keyring',
        'SHLVL': '0',
        'LC_IDENTIFICATION': 'en_IN.UTF-8',
        'LC_MONETARY': 'en_IN.UTF-8',
        'QT_IM_MODULE': 'ibus',
        'XDG_CONFIG_DIRS': '/etc/xdg/xdg-ubuntu:/etc/xdg',
        'LANG': 'en_US.UTF-8',
        'XDG_SESSION_TYPE': 'x11',
        'DISPLAY': ':0',
        'LC_NAME': 'en_IN.UTF-8',
        'CONDA_SHLVL': '1',
        'XDG_SESSION_CLASS': 'user',
        'GPG_AGENT_INFO': '/run/user/1000/gnupg/S.gpg-agent:0:1',
        'DESKTOP_SESSION': 'ubuntu',
        'USER': 'susnato',
        'XDG_MENU_PREFIX': 'gnome-',
        'GIO_LAUNCHED_DESKTOP_FILE_PID': '5455',
        'QT_ACCESSIBILITY': '1',
        'WINDOWPATH': '2',
        'LC_NUMERIC': 'en_IN.UTF-8',
        'GJS_DEBUG_OUTPUT': 'stderr',
        'SSH_AUTH_SOCK': '/run/user/1000/keyring/ssh',
        'GNOME_SHELL_SESSION_MODE': 'ubuntu',
        'XDG_RUNTIME_DIR': '/run/user/1000',
        'HOME': '/home/susnato',
        'PYDEVD_USE_FRAME_EVAL': 'NO',
        'JPY_PARENT_PID': '7479',
        'TERM': 'xterm-color',
        'CLICOLOR': '1',
        'FORCE_COLOR': '1',
        'CLICOLOR_FORCE': '1',
        'PAGER': 'cat',
        'GIT_PAGER': 'cat',
        'MPLBACKEND': 'module://matplotlib_inline.backend_inline',
        'LD_LIBRARY_PATH': '/usr/local/cuda-12.3/targets/x86_64-linux/lib/stubs/libcuda.so',
        'TF_CPP_MIN_LOG_LEVEL': '1',
        'KMP_DUPLICATE_LIB_OK': 'True',
        'KMP_INIT_AT_FORK': 'FALSE',
        'TPU_ML_PLATFORM': 'Tensorflow',
        'TF2_BEHAVIOR': '1'}
import os
os.environ['LD_LIBRARY_PATH'] = '/home/susnato/anaconda3/envs/phi/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda121.so'


from transformers import ViTForImageClassification, TrainingArguments, Trainer

checkpoint = "google/vit-base-patch16-224"
id2label = {i:cls for i, cls in enumerate(beans['train'].features['labels'].names)}
label2id = {v:k for k, v in id2label}

model = ViTForImageClassification.from_pretrained(
    checkpoint,
    torch_dtype=torch.float16,
    num_labels=len(labels),
    id2label=id2label,
    label2id=label2id,
)
2023-11-27 21:09:44.972363: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/susnato/anaconda3/envs/phi/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda123.so
False
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 123
CUDA SETUP: Required library version not found: libbitsandbytes_cuda123.so. Maybe you need to compile it from source?
CUDA SETUP: Defaulting to libbitsandbytes_cpu.so...

================================================ERROR=====================================
CUDA SETUP: CUDA detection failed! Possible reasons:
1. CUDA driver not installed
2. CUDA not installed
3. You have multiple conflicting CUDA libraries
4. Required library not pre-compiled for this bitsandbytes release!
CUDA SETUP: If you compiled from source, try again with `make CUDA_VERSION=DETECTED_CUDA_VERSION` for example, `make CUDA_VERSION=113`.
CUDA SETUP: The CUDA version for the compile might depend on your conda install. Inspect CUDA version via `conda list | grep cuda`.
================================================================================

CUDA SETUP: Something unexpected happened. Please compile from source:
git clone git@github.com:TimDettmers/bitsandbytes.git
cd bitsandbytes
CUDA_VERSION=123
python setup.py install
CUDA SETUP: Setup Failed!
/home/susnato/anaconda3/envs/phi/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/susnato/anaconda3/envs/phi did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/susnato/anaconda3/envs/phi/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/susnato/anaconda3/envs/phi/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda121.so did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/susnato/anaconda3/envs/phi/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('@/tmp/.ICE-unix/1318,unix/susnato-desktop'), PosixPath('local/susnato-desktop')}
  warn(msg)
/home/susnato/anaconda3/envs/phi/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/etc/xdg/xdg-ubuntu')}
  warn(msg)
/home/susnato/anaconda3/envs/phi/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('0'), PosixPath('1')}
  warn(msg)
/home/susnato/anaconda3/envs/phi/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}
  warn(msg)
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
File ~/temp_files/transformers/src/transformers/utils/import_utils.py:1353, in _LazyModule._get_module(self, module_name)
   1352 try:
-> 1353     return importlib.import_module("." + module_name, self.__name__)
   1354 except Exception as e:

File ~/anaconda3/envs/phi/lib/python3.9/importlib/__init__.py:127, in import_module(name, package)
    126         level += 1
--> 127 return _bootstrap._gcd_import(name[level:], package, level)

File <frozen importlib._bootstrap>:1030, in _gcd_import(name, package, level)

File <frozen importlib._bootstrap>:1007, in _find_and_load(name, import_)

File <frozen importlib._bootstrap>:986, in _find_and_load_unlocked(name, import_)

File <frozen importlib._bootstrap>:680, in _load_unlocked(spec)

File <frozen importlib._bootstrap_external>:850, in exec_module(self, module)

File <frozen importlib._bootstrap>:228, in _call_with_frames_removed(f, *args, **kwds)

File ~/temp_files/transformers/src/transformers/trainer.py:190
    189 if is_peft_available():
--> 190     from peft import PeftModel
    193 if is_accelerate_available():

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/peft/__init__.py:22
     20 __version__ = "0.5.0"
---> 22 from .auto import (
     23     AutoPeftModel,
     24     AutoPeftModelForCausalLM,
     25     AutoPeftModelForSequenceClassification,
     26     AutoPeftModelForSeq2SeqLM,
     27     AutoPeftModelForTokenClassification,
     28     AutoPeftModelForQuestionAnswering,
     29     AutoPeftModelForFeatureExtraction,
     30 )
     31 from .mapping import (
     32     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,
     33     PEFT_TYPE_TO_CONFIG_MAPPING,
   (...)
     36     inject_adapter_in_model,
     37 )

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/peft/auto.py:31
     30 from .config import PeftConfig
---> 31 from .mapping import MODEL_TYPE_TO_PEFT_MODEL_MAPPING
     32 from .peft_model import (
     33     PeftModel,
     34     PeftModelForCausalLM,
   (...)
     39     PeftModelForTokenClassification,
     40 )

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/peft/mapping.py:23
     22 from .config import PeftConfig
---> 23 from .peft_model import (
     24     PeftModel,
     25     PeftModelForCausalLM,
     26     PeftModelForFeatureExtraction,
     27     PeftModelForQuestionAnswering,
     28     PeftModelForSeq2SeqLM,
     29     PeftModelForSequenceClassification,
     30     PeftModelForTokenClassification,
     31 )
     32 from .tuners import (
     33     AdaLoraConfig,
     34     AdaLoraModel,
   (...)
     42     PromptTuningConfig,
     43 )

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/peft/peft_model.py:38
     37 from .config import PeftConfig
---> 38 from .tuners import (
     39     AdaLoraModel,
     40     AdaptionPromptModel,
     41     IA3Model,
     42     LoraModel,
     43     PrefixEncoder,
     44     PromptEmbedding,
     45     PromptEncoder,
     46 )
     47 from .utils import (
     48     SAFETENSORS_WEIGHTS_NAME,
     49     TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING,
   (...)
     62     shift_tokens_right,
     63 )

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/peft/tuners/__init__.py:21
     20 from .adaption_prompt import AdaptionPromptConfig, AdaptionPromptModel
---> 21 from .lora import LoraConfig, LoraModel
     22 from .ia3 import IA3Config, IA3Model

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/peft/tuners/lora.py:45
     44 if is_bnb_available():
---> 45     import bitsandbytes as bnb
     48 @dataclass
     49 class LoraConfig(PeftConfig):

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/bitsandbytes/__init__.py:6
      1 # Copyright (c) Facebook, Inc. and its affiliates.
      2 #
      3 # This source code is licensed under the MIT license found in the
      4 # LICENSE file in the root directory of this source tree.
----> 6 from . import cuda_setup, utils, research
      7 from .autograd._functions import (
      8     MatmulLtState,
      9     bmm_cublas,
   (...)
     13     matmul_4bit
     14 )

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/bitsandbytes/research/__init__.py:1
----> 1 from . import nn
      2 from .autograd._functions import (
      3     switchback_bnb,
      4     matmul_fp8_global,
      5     matmul_fp8_mixed,
      6 )

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/bitsandbytes/research/nn/__init__.py:1
----> 1 from .modules import LinearFP8Mixed, LinearFP8Global

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/bitsandbytes/research/nn/modules.py:8
      7 import bitsandbytes as bnb
----> 8 from bitsandbytes.optim import GlobalOptimManager
      9 from bitsandbytes.utils import OutlierTracer, find_outlier_dims

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/bitsandbytes/optim/__init__.py:6
      1 # Copyright (c) Facebook, Inc. and its affiliates.
      2 #
      3 # This source code is licensed under the MIT license found in the
      4 # LICENSE file in the root directory of this source tree.
----> 6 from bitsandbytes.cextension import COMPILED_WITH_CUDA
      8 from .adagrad import Adagrad, Adagrad8bit, Adagrad32bit

File ~/anaconda3/envs/phi/lib/python3.9/site-packages/bitsandbytes/cextension.py:20
     19     CUDASetup.get_instance().print_log_stack()
---> 20     raise RuntimeError('''
     21     CUDA Setup failed despite GPU being available. Please run the following command to get more information:
     22 
     23     python -m bitsandbytes
     24 
     25     Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them
     26     to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes
     27     and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues''')
     28 lib.cadam32bit_grad_fp32 # runs on an error if the library could not be found -> COMPILED_WITH_CUDA=False

RuntimeError: 
        CUDA Setup failed despite GPU being available. Please run the following command to get more information:

        python -m bitsandbytes

        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them
        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes
        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues

The above exception was the direct cause of the following exception:

RuntimeError                              Traceback (most recent call last)
Cell In[1], line 5
      1 import os
      2 os.environ['LD_LIBRARY_PATH'] = '/home/susnato/anaconda3/envs/phi/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda121.so'
----> 5 from transformers import ViTForImageClassification, TrainingArguments, Trainer
      7 checkpoint = "google/vit-base-patch16-224"
      8 id2label = {i:cls for i, cls in enumerate(beans['train'].features['labels'].names)}

File <frozen importlib._bootstrap>:1055, in _handle_fromlist(module, fromlist, import_, recursive)

File ~/temp_files/transformers/src/transformers/utils/import_utils.py:1343, in _LazyModule.__getattr__(self, name)
   1341     value = self._get_module(name)
   1342 elif name in self._class_to_module.keys():
-> 1343     module = self._get_module(self._class_to_module[name])
   1344     value = getattr(module, name)
   1345 else:

File ~/temp_files/transformers/src/transformers/utils/import_utils.py:1355, in _LazyModule._get_module(self, module_name)
   1353     return importlib.import_module("." + module_name, self.__name__)
   1354 except Exception as e:
-> 1355     raise RuntimeError(
   1356         f"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its"
   1357         f" traceback):\n{e}"
   1358     ) from e

RuntimeError: Failed to import transformers.trainer because of the following error (look up to see its traceback):

        CUDA Setup failed despite GPU being available. Please run the following command to get more information:

        python -m bitsandbytes

        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them
        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes
        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues
training_args = TrainingArguments(
    output_dir="my_awesome_food_model",
    remove_unused_columns=False,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    gradient_accumulation_steps=4,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    warmup_ratio=0.1,
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=beans["train"],
    eval_dataset=beans["validation"],
    tokenizer=image_processor,
    compute_metrics=compute_metrics,
)

trainer.train()
beans
DatasetDict({
    train: Dataset({
        features: ['image_file_path', 'image', 'labels'],
        num_rows: 1034
    })
    validation: Dataset({
        features: ['image_file_path', 'image', 'labels'],
        num_rows: 133
    })
    test: Dataset({
        features: ['image_file_path', 'image', 'labels'],
        num_rows: 128
    })
})
 
 
 
 
 
 
transforms(beans['train'])[0]#['image']
{'image_file_path': '/home/susnato/.cache/huggingface/datasets/downloads/extracted/38e29d6694476ac890e245695ef2a1dad6dc2327c8d4d9fc64a0b8e177b752f4/train/angular_leaf_spot/angular_leaf_spot_train.0.jpg',
 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500>,
 'labels': 0}
beans['train'].features['labels']
ClassLabel(names=['angular_leaf_spot', 'bean_rust', 'healthy'], id=None)
 
 
 
